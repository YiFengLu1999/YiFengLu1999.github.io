---
layout:     post
title:      Why Machines Learn 第一章读书笔记：从MCP到感知机，学习为何会“卡死”
subtitle:   线性可分的承诺、线性不可分的死循环，以及把逻辑变成几何直觉
date:       2026-01-11
author:     BY
header-img: img_Blog/neuro.jpg
catalog:    true
tags:
    - 读书笔记
    - 机器学习
    - 感知机
    - 神经网络
    - 数学直觉
---

# 这本书在做什么

《**Why Machines Learn: The Elegant Mathematics Behind Modern AI**》从感知机、核方法、概率与优化，一路讲到深度学习与大型语言模型。它的主旨不是堆公式，而是帮助读者建立一种更“顺手”的连接：

> **数学模型 = 几何 / 物理直觉**  

也就是：你看到一个模型，不只会算，还能在脑子里“看见”它在空间里怎么动、为什么动、会卡在哪里。

作者通常指 **Anil Ananthaswamy**：一位写作横跨物理学、神经科学与人工智能的科普作家。书的叙述风格也很像他的长项：从故事与直觉出发，再把数学推上台面。

---

# 第一章的主线：学习 = 快速找到模式

第一章给我的核心印象是：**“快速找到模式”本身就是一种高级智能**。

- 生物学习往往不需要海量数据，就能抓到规律（至少在很多日常任务上）。
- 于是一个很自然的科学野心出现了：  
  **如果我们能训练机器获得智能，也许能反过来理解人类智能是怎么来的。**

这就把两个主题绑在一起了：  
1) 计算与逻辑能否“长”出智能？  
2) 学习算法的数学结构，会不会映射到某种生物机制？

---

# 概念补课：线性方程、系数（权重）、回归

这一章频繁出现“线性”和“权重”，我用最小必要解释把它们钉牢。

## 1) 什么是线性方程？什么是系数（权重）？

一个典型的线性表达式是：

\[
z = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b
\]

- \(x_i\)：输入特征（你喂给模型的数字）
- \(w_i\)：系数，也叫**权重**（模型“偏好”哪个特征）
- \(b\)：偏置（把整体阈值平移一下）

**几何直觉**：  
这行式子在高维空间里对应一个“超平面”（二维是直线、三维是平面）。模型做分类，本质是在问：点在这条“分界线”的哪一边？

## 2) 什么是回归？

- **回归（regression）**：输出是连续数值（比如房价、温度、神经元放电率预测值）。
- **分类（classification）**：输出是离散标签（比如猫/狗、正/负、放电/不放电）。

感知机主要是分类故事：输出通常是 \(+1\) 或 \(-1\)。

---

# 计算与逻辑：MCP 神经元把 OR/AND 写进了阈值

第一章很迷人的地方在于：它不是一上来就“神经网络很强”，而是从**逻辑门**讲起——因为这是“计算”的最小积木。

## 什么是 MCP 神经元？

MCP（McCulloch–Pitts）神经元是一个非常早期、非常理想化的模型：

1. 把输入加权求和：\(s = \sum w_i x_i\)
2. 和阈值 \(\theta\) 比较：  
   - 若 \(s \ge \theta\)，输出 1（或放电）
   - 否则输出 0

用合适的 \(w\) 和 \(\theta\)，它可以实现 **OR**、**AND** 等逻辑门。  
这件事意义很大：**逻辑与计算可以被“神经元式的算子”实现**——计算与逻辑之间确实存在深刻的联系。

## MCP 的局限：\(\theta\) 不能通过学习获得

MCP 最大的问题是：阈值 \(\theta\) 通常是“人手设计”的。  
也就是说它能算，但不太会**自己变聪明**。

这就引出一个自然的问题：  
> 能不能让系统通过数据，自己把 \(w\) 和 \(\theta\) 学出来？

---

# 为什么会有人想到 Hebbian learning？

Hebb（赫布）提出了一个极具直觉的学习想法（后来影响深远）：

> **当一个神经元的输出持续参与到另一个神经元的放电过程中时，它们之间的连接会增强；反之则会减弱。**

一言以蔽之：**一起放电，就连得更牢。**

从本章的叙事逻辑看，Hebbian learning 很像是在回应 MCP 的“不可学习性”：  
既然 MCP 的阈值/连接不是从经验里来的，那我们就需要某种“从经验里调整连接强度”的规则。

---

# 感知机：把“可学习”这件事做成算法

感知机（Perceptron）在本章里是一位关键主角，因为它给出一个非常干净的承诺：

> **只要答案存在（数据线性可分），给我点时间，我一定会找到它。**

## 感知机是什么？它和 MCP 有什么不同？

直觉上可以这样记：

- MCP：能算逻辑，但参数（尤其阈值）更多是设计出来的  
- 感知机：把参数当作“要学习的对象”，用数据驱动去改

很多讲法里会把偏置 \(b\)（或阈值 \(\theta\)）并入模型一起学。  
这点很关键：**阈值能学**，模型才真的像在“适应环境”。

## 感知机怎么学习？——通过错误的积累

感知机的学习规则极其朴素，甚至有点“倔强”：

- 只要分错了，就调整参数，让它下次更倾向分对。
- 调整方向由“错在哪里”决定。

一种常见写法（用 \(y\in\{+1,-1\}\) 表示真标签）是：  
若样本 \(x\) 被分错，则

\[
w \leftarrow w + yx,\quad b \leftarrow b + y
\]

**直觉**：  
- 如果本该是正类却被判成负类，你就把分界线往“包含它”的方向推一推；
- 如果本该是负类却被判成正类，你就往相反方向推一推。

---

# 线性可分：感知机的“收敛承诺”

书里强调的一句非常重要：

- **当数据是线性可分（linearly separable）时，感知机一定会在有限步内收敛。**

这意味着什么？

- 你不是在赌运气；
- 你得到一个数学保证：**只要存在一条能把两类分开的直线/超平面，感知机会找到它。**

这是早期机器学习特别珍贵的“硬承诺”。

---

# 线性不可分：红豆绿豆与“永无止境的纠结”

然后，故事变得很人类（笑）。

想象一下：红豆和绿豆混在一起，你怎么画一条直线都无法把它们切开——这就是**线性不可分**。

此时感知机会进入一种非常折磨人的循环：

1. 它画了一根线，发现漏掉一个红点，于是赶紧调整角度。  
2. 结果一调整，又把原本对的一个绿点划错了。  
3. 它又赶紧去补救那个绿点，结果之前的红点又错了。

关键在于：  
感知机的设计是“只要有错就必须改”，但在不可分问题上，**你改哪儿都会制造新的错**。  

于是它会在数据堆里“跳舞”，直到世界末日，也不会停下来告诉你一句——**“这个问题没法做。”**

## 这暴露了什么缺陷？

感知机缺乏一种**全局视角**：

- 它很擅长“局部修错”
- 但它无法判断“是否存在正确答案”
- 所以当答案不存在时，它不会优雅失败，只会无限挣扎

这也是为什么后续机器学习会发展出：
- 更强的目标函数（有全局度量）
- 更稳定的优化方法
- 以及能处理不可分情况的模型（比如引入非线性、核方法、多层网络等）

---

# 我从第一章带走的几个钉子（很硬的那种）

1. **学习算法的“行为”是几何的**：分界线怎么转、怎么平移，比公式更先让你理解发生了什么。  
2. **感知机的伟大在于“可证明的收敛”**：只要线性可分，它一定会抓住答案。  
3. **感知机的痛苦在于“不可分时不会认输”**：它没有机制判断任务本身是否可解。  
4. **从 MCP 到感知机，是从“手工设置阈值”到“阈值可学习”的跃迁**：这一步非常关键，决定了模型是否真的能适应数据。

---

# 一点前瞻：为什么这章像“起源神话”？

第一章的感受像是在看一个宇宙起源故事：  
它把后面一切复杂模型（核方法、深度网络、甚至大模型）都会反复遇到的问题，提前埋了种子：

- 线性 vs 非线性  
- 局部修错 vs 全局优化  
- 可解性、收敛、以及“如何优雅地失败”

后面的章节，本质上都在给第一章的缺陷“升级装备”。

